<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="NLP领域的对抗式方法综述"><meta name="keywords" content="NLP,Adversarial-based"><meta name="author" content="yym6472"><meta name="copyright" content="yym6472"><title>NLP领域的对抗式方法综述 | yym6472's Blog</title><link rel="shortcut icon" href="/favicon-256.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"G2KYQO0W55","apiKey":"f8377236a1dbdcb6cfe6bcce269cbc82","indexName":"blog","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#论文1对抗式方法解决多标准下的中文分词问题"><span class="toc-number">1.</span> <span class="toc-text"> 论文1：对抗式方法解决多标准下的中文分词问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#本文拟解决的问题"><span class="toc-number">1.1.</span> <span class="toc-text"> 本文拟解决的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#本文提出的模型"><span class="toc-number">1.2.</span> <span class="toc-text"> 本文提出的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验结果"><span class="toc-number">1.3.</span> <span class="toc-text"> 实验结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#相关引文"><span class="toc-number">1.4.</span> <span class="toc-text"> 相关引文</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#论文2对抗式方法解决文本风格迁移问题"><span class="toc-number">2.</span> <span class="toc-text"> 论文2：对抗式方法解决文本风格迁移问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#本文拟解决的问题-2"><span class="toc-number">2.1.</span> <span class="toc-text"> 本文拟解决的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#本文的贡献"><span class="toc-number">2.2.</span> <span class="toc-text"> 本文的贡献</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#相关工作-引文"><span class="toc-number">2.3.</span> <span class="toc-text"> 相关工作、引文</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#图像领域的风格迁移"><span class="toc-number">2.3.1.</span> <span class="toc-text"> 图像领域的风格迁移</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#文本领域的风格迁移"><span class="toc-number">2.3.2.</span> <span class="toc-text"> 文本领域的风格迁移</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#adversarial-networks-for-domain-separation"><span class="toc-number">2.3.3.</span> <span class="toc-text"> Adversarial Networks for Domain Separation</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#总结"><span class="toc-number">2.3.4.</span> <span class="toc-text"> 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#本文提出的模型-2"><span class="toc-number">2.4.</span> <span class="toc-text"> 本文提出的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#本文提出的风格迁移的评价方法"><span class="toc-number">2.5.</span> <span class="toc-text"> 本文提出的风格迁移的评价方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#transfer-strength"><span class="toc-number">2.5.1.</span> <span class="toc-text"> Transfer Strength</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#content-preservation"><span class="toc-number">2.5.2.</span> <span class="toc-text"> Content Preservation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验结果-2"><span class="toc-number">2.6.</span> <span class="toc-text"> 实验结果</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#数据集"><span class="toc-number">2.6.1.</span> <span class="toc-text"> 数据集</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实验结果-3"><span class="toc-number">2.6.2.</span> <span class="toc-text"> 实验结果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#论文3对抗式方法解决针对序列任务的主动学习算法"><span class="toc-number">3.</span> <span class="toc-text"> 论文3：对抗式方法解决针对序列任务的主动学习算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结和思考"><span class="toc-number">4.</span> <span class="toc-text"> 总结和思考</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://i.loli.net/2019/04/27/5cc4218c74e0e.jpg"></div><div class="author-info__name text-center">yym6472</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/yym6472">Follow Me on GitHub</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">5</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">6</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">4</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://molunerfinn.com/">MARKSZのBlog (Theme Auther's Blog)</a><a class="author-info-links__name text-center" href="https://pris-nlp.github.io/">BUPT PRIS Lab, NLP Group</a><a class="author-info-links__name text-center" href="https://helicqin.github.io/">Helicqin's Blog</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2019/04/27/5cc456373162f.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">yym6472's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a><a class="site-page" href="/contact">Contact</a></span></div><div id="post-info"><div id="post-title">NLP领域的对抗式方法综述</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-04-20</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习/">学习</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习/论文阅读笔记/">论文阅读笔记</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2.9k</span><span class="post-meta__separator">|</span><span>Reading time: 9 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>这篇文章将基于以下三篇论文，对NLP领域中应用<strong>对抗式方法</strong>解决相关问题的模型、算法做一个综述：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1704.07556.pdf" target="_blank" rel="noopener">Chen et al. 2017. Adversarial Multi-Criteria Learning for Chinese Word Segmentation</a></li>
<li><a href="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17015/15745" target="_blank" rel="noopener">Fu et al. 2018. Style Transfer in Text: Exploration and Evaluation. AAAI-18</a></li>
<li><a href="https://www.ijcai.org/proceedings/2018/0558.pdf" target="_blank" rel="noopener">Deng et al. 2018. Adversarial Active Learning for Sequence Labeling and Generation. IJCAI-18</a></li>
</ul>
<a id="more"></a>
<h2 id="论文1对抗式方法解决多标准下的中文分词问题"><a class="markdownIt-Anchor" href="#论文1对抗式方法解决多标准下的中文分词问题"></a> 论文1：对抗式方法解决多标准下的中文分词问题</h2>
<h3 id="本文拟解决的问题"><a class="markdownIt-Anchor" href="#本文拟解决的问题"></a> 本文拟解决的问题</h3>
<p>在中文分词领域中，常常存在许多不同的分词标准（criteria），如下图所示：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/f27e11e6-5f33-11e9-888b-0242ac110002.jpg" alt="不同的分词标准"><br>
可以看到在上面两个语料集中，使用了不同的标准对同一句话“姚明进入总决赛”进行分词，得到了不同的结果。</p>
<p>此前的大部分方法都是针对单一分词标准进行的分词模型研究，本文提出的模型旨在将不同标准的语料库一起利用起来，进行多任务联合训练，通过对公共特征的学习，帮助提升各个子任务的分词性能。</p>
<h3 id="本文提出的模型"><a class="markdownIt-Anchor" href="#本文提出的模型"></a> 本文提出的模型</h3>
<p>本文提出的多任务（多分词标准）联合训练模型如下图所示：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/49fc486a-5f35-11e9-888b-0242ac110002.jpg" alt="本文提出的模型（三种变体）"></p>
<p>其中：黄色的方框是共享的Bi-LSTM层，灰色的方框是每个任务私有的Bi-LSTM层。黄色的圆圈是多任务共享的word-embedding层。<br>
这三个模型（Model-I ~ Model-III）的差别就是共享Bi-LSTM层数据的流向不同，如图中的红色箭头所示。虽然图中只放了两个Task，但实际上可以有多个Task，它们分别有各自的Bi-LSTM层和CRF层，并且使用共享Bi-LSTM层提供的信息。</p>
<p>多任务联合训练模型（需要最大化）的目标函数如下：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/b8bed532-5f36-11e9-95f5-0242ac110002.jpg" alt="task loss"><br>
这个目标函数是面向任务的，论文中将其称为<strong>task loss</strong>。</p>
<hr>
<p>在上述多任务联合训练模型的基础上，作者受到一些对抗式模型（共三篇，见引文）的启发，加入了对抗判别器如下：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/f7504ede-5f36-11e9-888b-0242ac110002.jpg" alt="加入了对抗判别器后的模型"><br>
其结构为：将输入句子通过共享Bi-LSTM层，将每个时间步的hidden state取平均，然后过一个线性映射，再过一个softmax，最后向量中的每个值就表示对应的各个任务的分数。</p>
<p>对抗判别器的目标是：<strong>正确区分出给定sentence属于哪一个任务</strong>。因此其目标函数如下：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/dac03f6e-5f3a-11e9-888b-0242ac110002.jpg" alt="判别器的adversarial loss"></p>
<p>而shared Bi-LSTM的目标是和判别器对抗的，即：<strong>不让判别器区分出给定sentence属于哪一个任务</strong>，因此其目标函数如下：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/255e45f2-5f3b-11e9-888b-0242ac110002.jpg" alt="shared Bi-LSTM的adversarial loss"></p>
<p>以上两者是<strong>判别器和shared Bi-LSTM相互对抗</strong>的loss，作者称其为<strong>adversarial loss</strong>。</p>
<p>最终任务的损失函数如下：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/7afb1954-5f3b-11e9-888b-0242ac110002.jpg" alt="total loss"></p>
<p>训练过程是交替进行的，如下：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/8bb7bb76-5f3b-11e9-95f5-0242ac110002.jpg" alt="交替训练的算法"></p>
<h3 id="实验结果"><a class="markdownIt-Anchor" href="#实验结果"></a> 实验结果</h3>
<p><img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/db08a172-5f3b-11e9-888b-0242ac110002.jpg" alt="实验结果"></p>
<h3 id="相关引文"><a class="markdownIt-Anchor" href="#相关引文"></a> 相关引文</h3>
<p>论文中引用的三篇做对抗式的论文：</p>
<ul>
<li><a href="https://arxiv.org/pdf/1412.4446" target="_blank" rel="noopener">Ajakan et al. 2014. Domain-Adversarial Neural Networks.</a></li>
<li><a href="http://www.jmlr.org/papers/volume17/15-239/15-239.pdf" target="_blank" rel="noopener">Ganin et al. 2016. Domain-Adversarial Training of Neural Networks.</a></li>
<li><a href="http://papers.nips.cc/paper/6254-domain-separation-networks.pdf" target="_blank" rel="noopener">Bousmalis et al. 2016. Domain Separation Networks.</a></li>
</ul>
<h2 id="论文2对抗式方法解决文本风格迁移问题"><a class="markdownIt-Anchor" href="#论文2对抗式方法解决文本风格迁移问题"></a> 论文2：对抗式方法解决文本风格迁移问题</h2>
<h3 id="本文拟解决的问题-2"><a class="markdownIt-Anchor" href="#本文拟解决的问题-2"></a> 本文拟解决的问题</h3>
<p>图像领域的风格迁移已经取得进展，但文本领域的风格迁移相比图像领域进展缓慢，作者认为有以下两个原因：</p>
<ul>
<li>缺少平行数据（相同语义、不同表述风格的数据集）</li>
<li>缺少可靠的评价方法<br>
本文就尝试解决文本领域风格迁移的上述两个问题，从而推动文本领域风格迁移的发展。</li>
</ul>
<h3 id="本文的贡献"><a class="markdownIt-Anchor" href="#本文的贡献"></a> 本文的贡献</h3>
<p>本文的贡献在以下三个方面：</p>
<ul>
<li>提出了一个<a href="https://github.com/fuzhenxin/textstyletransferdata" target="_blank" rel="noopener">论文风格-新闻风格的标题数据集</a>以促进文本领域风格迁移的研究</li>
<li>提出了文本风格迁移两个评价方法：风格转换能力、内容保留度，并且作者认为这两个评价指标和人类的标准高度吻合</li>
<li>提出了文本风格迁移的两个模型，并在上述评价方法上进行验证，发现两个模型在不同情形下各有所长</li>
</ul>
<h3 id="相关工作-引文"><a class="markdownIt-Anchor" href="#相关工作-引文"></a> 相关工作、引文</h3>
<h4 id="图像领域的风格迁移"><a class="markdownIt-Anchor" href="#图像领域的风格迁移"></a> 图像领域的风格迁移</h4>
<ul>
<li>分离图像的内容和风格信息，然后将它们重新融合以生成新图片：<a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf" target="_blank" rel="noopener">Gatys, L. A.; Ecker, A. S.; and Bethge, M. 2016. Image style transfer using convolutional neural networks. In <em>Pro-ceedings of the IEEE Conference on Computer Vision andPattern Recognition</em>, 2414–2423.</a></li>
<li>使用一个简单的线性模型来改变图片的颜色（or色调?）：<a href="https://arxiv.org/pdf/1606.05897" target="_blank" rel="noopener">Gatys, L. A.; Bethge, M.; Hertzmann, A.; and Shechtman, E. 2016. Preserving color in neural artistic style transfer. <em>arXiv preprint arXiv:1606.05897</em>.</a></li>
<li>CycleGAN（有点像自编解码网络）：<a href="https://arxiv.org/pdf/1703.10593" target="_blank" rel="noopener">Zhu, J.-Y.; Park, T.; Isola, P.; and Efros, A. A. 2017. Un-paired image-to-image translation using cycle-consistent adversarial networks. <em>arXiv preprint arXiv:1703.10593</em>.</a></li>
<li>将图像风格迁移视作领域迁移：<a href="https://arxiv.org/pdf/1701.01036" target="_blank" rel="noopener">Li, Y.; Wang, N.; Liu, J.; and Hou, X. 2017. Demystifying neural style transfer. <em>InIJCAI</em>.</a></li>
</ul>
<h4 id="文本领域的风格迁移"><a class="markdownIt-Anchor" href="#文本领域的风格迁移"></a> 文本领域的风格迁移</h4>
<ul>
<li>用到了<a href="https://arxiv.org/abs/1506.03134" target="_blank" rel="noopener">Pointer Networks</a>将现代英语翻译成莎士比亚英语：</li>
<li>还提到了一些别的论文，但我感觉没必要看（也看不懂），涉及到两篇使用VAE做生成的。</li>
</ul>
<h4 id="adversarial-networks-for-domain-separation"><a class="markdownIt-Anchor" href="#adversarial-networks-for-domain-separation"></a> Adversarial Networks for Domain Separation</h4>
<p>这其实就是这篇综述中，三篇论文类似的思路，只是适用范围变广了（多分词标准、风格转换、迁移学习、多任务学习等等），实际上本质上还是相同的（目的都是为了学习到公共的、与具体xxx无关的知识）。其实最早在2015年就有人用这样的思路做Domain Separation了。</p>
<h4 id="总结"><a class="markdownIt-Anchor" href="#总结"></a> 总结</h4>
<p>这篇文章的引文部分内容还挺多的，可以之后有时间了看一看，顺便多了解一些生成领域的知识。其中提到的一些比较有意思的模型有：</p>
<ul>
<li>Pointer Networks, 主要用于解决seq2seq问题中，<a href="https://www.jianshu.com/p/bff3a19c59be" target="_blank" rel="noopener">decoder端的输出项依赖encoder的输入项</a>的问题</li>
<li>CycleGAN，具体还没有看是用来干啥的</li>
<li>VAE，貌似涉及理论的东西比较多，很难看懂…</li>
</ul>
<h3 id="本文提出的模型-2"><a class="markdownIt-Anchor" href="#本文提出的模型-2"></a> 本文提出的模型</h3>
<p>本文提出了两种不同的模型，这两种模型的不同只是在于Decoder端，其他的部分（包括对抗判别器）都是相同的。<br>
模型一（图左）对每个style单独训练一个decoder；而模型二（图右）只训练一个共同的decoder，通过lookup的方式获取不同style的style embedding（类似word embedding），并将其传入共同的这一个decoder。</p>
<p><img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/687e5642-5f40-11e9-888b-0242ac110002.jpg" alt="本文提出的两种模型"></p>
<p>这两个模型都采用了传统的做seq2seq的Encoder-Decoder结构作为主体。模型假设：Encoder只提取出<strong>文本的内容表示</strong>，而不会提取文本的风格信息；而Decoder基于Encoder提取出来的，本文的内容表示（<code>Content c</code>）<strong>去重构原始文本</strong>。由于之前保证了Encoder只会提取出文本的内容信息（而没有风格信息），那么Decoder相当于就能学习到不同style文本共有的style信息，<strong>在此style信息的基础上，对内容信息进行加工，从而重构出原始句子</strong>。</p>
<p>因此论文的<strong>task loss</strong>是要让Decoder的输出去恢复原始句子：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/19/22f313aa-627f-11e9-af5e-0242ac110002.jpg" alt="seq2seq模型的loss"><br>
其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是原始序列和目标序列，它们是相同的（i.e. <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub><mo>=</mo><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_i == y_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>）。论文称之为seq2seq的自编解码网络（<strong>auto-encoder seq2seq model</strong>）。</p>
<p>但是光有上面的Encoder-Decoder是无法保证Encoder单单提取出文本的内容信息的。因此这里就采用了对抗式的方法：引入了对抗判别器，接受Encoder编码后的向量，过一个多层感知器和softmax，目标是判别文本的风格（style）。</p>
<p>类似于论文1，这里的loss分成两个部分：<strong>task loss</strong>和<strong>adversarial loss</strong>，其中adversarial loss是成对出现的（分别属于对抗的两个主体）：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/9ecb41b4-5f41-11e9-888b-0242ac110002.jpg" alt="task loss"><br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/b3dc9292-5f41-11e9-888b-0242ac110002.jpg" alt="判别器的adversarial loss"><br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/c04b0f40-5f41-11e9-95f5-0242ac110002.jpg" alt="编码器的adversarial loss"><br>
基本和论文1的形式是相同的（只是论文1中是最大化目标函数，这里是最小化loss），因此这里不再展开讲。</p>
<p>最后针对两个模型的总的total loss如下：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/e257bba6-5f41-11e9-888b-0242ac110002.jpg" alt="第一种模型的total loss"><br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/15/08d73dc4-5f42-11e9-888b-0242ac110002.jpg" alt="第二种模型的total loss"><br>
分别属于两个模型。</p>
<h3 id="本文提出的风格迁移的评价方法"><a class="markdownIt-Anchor" href="#本文提出的风格迁移的评价方法"></a> 本文提出的风格迁移的评价方法</h3>
<h4 id="transfer-strength"><a class="markdownIt-Anchor" href="#transfer-strength"></a> Transfer Strength</h4>
<p>作者用100,000+数据训练了一个二分类的判别器（可以在<a href="https://github.com/fchollet/keras/blob/master/examples/imdblstm.py" target="_blank" rel="noopener">这里</a>获取到），去判别文本的风格（news OR paper）。该评价指标的计算方式为：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub><mi mathvariant="normal">/</mi><msub><mi>N</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{right} / N_{total}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.036108em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord">/</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mrow><mi>r</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{right}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02778em;">r</span><span class="mord mathdefault mtight">i</span><span class="mord mathdefault mtight" style="margin-right:0.03588em;">g</span><span class="mord mathdefault mtight">h</span><span class="mord mathdefault mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span></span></span></span>是正确迁移到目标风格的样本数量，而<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>N</mi><mrow><mi>t</mi><mi>o</mi><mi>t</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">N_{total}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">o</span><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">a</span><span class="mord mathdefault mtight" style="margin-right:0.01968em;">l</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是测试样本总数。</p>
<h4 id="content-preservation"><a class="markdownIt-Anchor" href="#content-preservation"></a> Content Preservation</h4>
<p>对于内容保留度，作者采用的是原句和迁移后句子的向量余弦距离。句子的向量被定义为一个三维向量，其每一项分别为对所有的word的embedding做max、mean、min pooling的结果，如下所示：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/19/8a6b0a76-6282-11e9-b495-0242ac110002.jpg" alt="内容保留度的计算方法"><br>
而word embedding采用的是stanford发布的Glove，使用了其中100维的word embedding。</p>
<h3 id="实验结果-2"><a class="markdownIt-Anchor" href="#实验结果-2"></a> 实验结果</h3>
<h4 id="数据集"><a class="markdownIt-Anchor" href="#数据集"></a> 数据集</h4>
<p>采用了两个数据集，分别是：news-paper title和positive-negative review。两者都是非平行（即没有news2paper一一对应关系）的数据。</p>
<h4 id="实验结果-3"><a class="markdownIt-Anchor" href="#实验结果-3"></a> 实验结果</h4>
<p>实验采用了普通的auto-encoder seq2seq model作为对照，在本文提出的两个指标上进行了文本迁移及其评价，结果如下图：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/19/53a4f4c8-6289-11e9-b495-0242ac110002.jpg" alt="实验结果图示"><br>
其中不同的结点代表了作者采用的不同的超参数设定。</p>
<p>下图为一些迁移的样本：<br>
<img src="https://media-cdn.jiuzhang.com/markdown/images/4/19/8be12f1e-6289-11e9-b495-0242ac110002.jpg" alt="迁移样本示例"><br>
可以看到在positive-negative迁移中的效果还是不错的。</p>
<h2 id="论文3对抗式方法解决针对序列任务的主动学习算法"><a class="markdownIt-Anchor" href="#论文3对抗式方法解决针对序列任务的主动学习算法"></a> 论文3：对抗式方法解决针对序列任务的主动学习算法</h2>
<p>这篇论文使用对抗要解决的问题就和上两篇论文（多任务、多领域等）有些不太一样。这篇论文使用对抗的目的是解决主动学习中的query sample selection问题，即如何从未标注的数据集中选取下一批要标注的数据。</p>
<p>具体内容详见<a href="https://yym6472.github.io/2019/04/08/%E3%80%8AAdversarial-Active-Learning-for-Sequence-Labeling-and-Generation%E3%80%8B%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/#%E4%B8%BB%E5%8A%A8%E5%AD%A6%E4%B9%A0active-learning%E4%BB%8B%E7%BB%8D">之前的笔记</a>。这里就不再重复。</p>
<h2 id="总结和思考"><a class="markdownIt-Anchor" href="#总结和思考"></a> 总结和思考</h2>
<p>Q：这种用对抗的方式做Domain Separation相比非对抗式的训练是否引入了新的信息呢？</p>
<p>A：我觉得是引入了的，就是<strong>各个样本取自哪个领域/任务</strong>的额外信息。在单纯采用多任务联合训练的模型，而不使用对抗的时候，模型是得不到<code>每个样本取自什么领域/任务</code>这一维度信息的，虽然看上去貌似不同领域/任务的样本被喂入不同的编码器，但模型是不知道它们之间的含义的。而当采用了对抗做Domain Separation后，判别器的损失函数中就包含这一维度的信息了。</p>
<p>其实这种对抗的方法相当于是让多任务联合训练的模型自动去学习了一种<strong>动态的</strong>损失函数，使得公共编码器尽量只学习公共知识。如果不使用对抗的话，这一点很难做到，因为编码器出来的本身就是无法解释的隐层向量，这个隐层向量中包含了多少的公共知识，包含了多少的领域知识，根本无法直接从这个向量中看出来，所以也就没办法直接对这个向量定义损失函数。而对抗学习了一个判别器，这个判别器能够找各种刁钻的角度去分析这个隐层向量，试图揪出其中一点点的特定领域的信息。所以这个判别器就相当于是对公共编码器定义了一个动态变化的损失函数了。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">yym6472</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://yym6472.github.io/2019/04/20/NLP领域的对抗式方法综述/">https://yym6472.github.io/2019/04/20/NLP领域的对抗式方法综述/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/Adversarial-based/">Adversarial-based</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/04/26/《Active-Learning-for-Deep-Semantic-Parsing》阅读笔记/"><i class="fa fa-chevron-left">  </i><span>《Active Learning for Deep Semantic Parsing》阅读笔记</span></a></div><div class="next-post pull-right"><a href="/2019/04/08/《Adversarial-Active-Learning-for-Sequence-Labeling-and-Generation》阅读笔记/"><span>《Adversarial Active Learning for Sequence Labeling and Generation》阅读笔记</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '1470654ab571587d5962',
  clientSecret: 'dd25cc8b92dbedf931e5a5e29b75f6bdbcff11ed',
  repo: 'yym6472.github.io',
  owner: 'yym6472',
  admin: 'yym6472',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(https://i.loli.net/2019/04/27/5cc456373162f.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By yym6472</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/algolia.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>