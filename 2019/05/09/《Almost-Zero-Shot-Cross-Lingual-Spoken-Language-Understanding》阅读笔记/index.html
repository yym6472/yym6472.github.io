<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="《(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding》阅读笔记"><meta name="keywords" content="NLP,Transfer Learning"><meta name="author" content="yym6472"><meta name="copyright" content="yym6472"><title>《(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding》阅读笔记 | yym6472's Blog</title><link rel="shortcut icon" href="/favicon-256.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"G2KYQO0W55","apiKey":"f8377236a1dbdcb6cfe6bcce269cbc82","indexName":"blog","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#本论文拟解决的问题"><span class="toc-number">1.</span> <span class="toc-text"> 本论文拟解决的问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#相关工作"><span class="toc-number">2.</span> <span class="toc-text"> 相关工作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#本论文提出的方法"><span class="toc-number">3.</span> <span class="toc-text"> 本论文提出的方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基础模型"><span class="toc-number">3.1.</span> <span class="toc-text"> 基础模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#aligned-word-embedding"><span class="toc-number">3.2.</span> <span class="toc-text"> aligned word embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#双语言共同训练"><span class="toc-number">3.3.</span> <span class="toc-text"> 双语言共同训练</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实验"><span class="toc-number">4.</span> <span class="toc-text"> 实验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#数据来源"><span class="toc-number">4.1.</span> <span class="toc-text"> 数据来源</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验结果"><span class="toc-number">4.2.</span> <span class="toc-text"> 实验结果</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://i.loli.net/2019/04/27/5cc4218c74e0e.jpg"></div><div class="author-info__name text-center">yym6472</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/yym6472">Follow Me on GitHub</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">13</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">14</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://molunerfinn.com/">MARKSZのBlog (Theme Auther's Blog)</a><a class="author-info-links__name text-center" href="https://pris-nlp.github.io/">BUPT PRIS Lab, NLP Group</a><a class="author-info-links__name text-center" href="https://helicqin.github.io/">Helicqin's Blog</a><a class="author-info-links__name text-center" href="https://hexo.io/">Hexo</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2019/04/27/5cc456373162f.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">yym6472's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a><a class="site-page" href="/contact">Contact</a></span></div><div id="post-info"><div id="post-title">《(Almost) Zero-Shot Cross-Lingual Spoken Language Understanding》阅读笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-09</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习/">学习</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习/论文阅读笔记/">论文阅读笔记</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.9k</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>Conference from: 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2018)</p>
<p>Paper link: <a href="http://shyamupa.com/papers/UFTHH18.pdf" target="_blank" rel="noopener">http://shyamupa.com/papers/UFTHH18.pdf</a></p>
<a id="more"></a>
<h2 id="本论文拟解决的问题"><a class="markdownIt-Anchor" href="#本论文拟解决的问题"></a> 本论文拟解决的问题</h2>
<p>SLU（Spoken Language Understanding）任务指的是分析、理解用户的自然语言输入，将其转化为机器能够理解的形式（包括意图识别和槽填充）。</p>
<p>本文旨在解决跨语言的SLU任务迁移。目前SOTA的SLU模型在英语领域已经获得了很高的性能，但是于其他语言，因为种种原因（数据不足等），仍然无法取得较高性能。</p>
<p>目前将英语SLU迁移到其他语言领域的方法主要是采用机器翻译技术，但是机器翻译对于一些低资源（low resource）的语言本身就不太可靠，因此本文主要想通过<strong>低成本、少标注</strong>的方法，解决SLU从英语迁移到其它低资源语言的问题。</p>
<p><img src="https://i.loli.net/2019/05/09/5cd38a294dba8.png" alt="跨语言的SLU任务示意图"></p>
<h2 id="相关工作"><a class="markdownIt-Anchor" href="#相关工作"></a> 相关工作</h2>
<p>之前的将SLU从英语迁移到其它语言的方法大多采用机器翻译，这里有两种方法：</p>
<ul>
<li>Test On Source</li>
<li>Train On Target<br>
<img src="https://i.loli.net/2019/05/09/5cd38d6a092b1.png" alt="采用机器翻译做跨语言SLU迁移的两种方法"></li>
</ul>
<p>前者（Test On Source）指的是在测试时，将目标语言翻译成英语，然后使用英语SLU模型完成SLU任务。后者（Train on Target）指的是在训练时将英语训练语料通过机器翻译，翻译成目标语言的训练语料，在此基础上训练一个目标语言的SLU模型。</p>
<p>这两种方法的缺点：</p>
<ul>
<li>都依赖于机器翻译，但是机器翻译在一些低资源语言上本身就是不可靠甚至无法训练的。此前的这一类工作的目标语言大多数都是流行的语言，例如汉语、法语、西班牙语等，英语-目标语言的翻译质量比较好。</li>
<li>在第一个方法（Test On Source）中，因为需要先把预测的文本翻译成英语，增加了额外的reference延迟</li>
<li>机器翻译训练的语料和SLU训练的语料不同（前者是任意的平行语料；后者是对话中的平行语料），两者具有领域差异，可能会带来不好的效果</li>
</ul>
<p>此外还提出了上述两个方法的一些变体，其中一个就是基于Test On Source方法的变体，它在训练英语SLU的时候不单单采用英语语料。而是将英语翻译成目标语言，再把目标语言反向翻译回英语，将通过这种方法得到的训练样本和原始的样本混合，在此基础上去训练英语SLU模型。其背后的启发就是：通过这种方法能够让SLU模型能够自适应处理机器翻译过程中带来的干扰，从而在做reference的时候能够避免翻译不准确带来的SLU识别错误。</p>
<h2 id="本论文提出的方法"><a class="markdownIt-Anchor" href="#本论文提出的方法"></a> 本论文提出的方法</h2>
<p>本文提出了一种能够将SLU模型从英语迁移到目标语言的方法，而无需依赖机器翻译系统。</p>
<h3 id="基础模型"><a class="markdownIt-Anchor" href="#基础模型"></a> 基础模型</h3>
<p>本文首先提出了SLU任务的基础模型，主要受到联合训练slot filling和intent classification模型的启发：<br>
<img src="https://i.loli.net/2019/05/09/5cd39a84e8853.png" alt="本文的基础模型"></p>
<p>基础模型就是简单地将word embedding后的向量序列经过一个双向LSTM，每一个时间步的隐层输出<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">h_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>经过softmax后就是对应的slot filling的序列；LSTM最终的状态<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">h_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>经过softmax后就是对应的intent。</p>
<h3 id="aligned-word-embedding"><a class="markdownIt-Anchor" href="#aligned-word-embedding"></a> aligned word embedding</h3>
<p>上面的基础模型能够在英语或目标语言上进行训练。为了利用能够轻易获取到的英语语料预训练来提高目标语言的性能（迁移学习），本文使用了<strong>aligned word embedding</strong>的方法。</p>
<p>这一技术的意思是，为了能在两种语言的语料上共享SLU模型参数，需要在词语表示的这一层（即word embedding层）就将两种语言的相同含义的词语在向量空间中的向量表示进行对齐。例如：英语“cloud”和中文的“云”，需要将两者映射到向量空间中相接近的地方（i.e. 两者的余弦相似度需要接近于1）。</p>
<p>Aligning word embedding此前也有了一些研究（见原文的引用），一种方法是使用两个线性变换矩阵<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold">W</mi></mrow><annotation encoding="application/x-tex">\bf{W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">W</span></span></span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="bold">V</mi></mrow><annotation encoding="application/x-tex">\bf{V}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68611em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbf" style="margin-right:0.01597em;">V</span></span></span></span></span></span>，分别对两个语言的word embedding矩阵做线性映射，映射到相同维度的向量空间中，要求在映射后的向量，两种语言中语义相同的词语具有相似的向量表示（两者的余弦相似度较高）。</p>
<p>本论文使用的word embedding模型来自<a href="https://www.mitpressjournals.org/doi/pdfplus/10.1162/tacl_a_00051" target="_blank" rel="noopener">论文《Enriching word vectors with subword information》</a>，其<a href="https://github.com/facebookresearch/fastText#resources" target="_blank" rel="noopener">github上的项目</a>提供了157种语言在维基百科上的预训练模型。在这些word embedding模型的基础上，基于<a href="https://arxiv.org/pdf/1702.03859" target="_blank" rel="noopener">论文《Offline bilingual word vectors, orthogonal transformations and the inverted softmax》(ICLR 2017)</a>的方法将其转化为双语言的aligned word embeddings。作者说也尝试了其它aligning embeddings的方法如CCA（论文《Improving vector space word representations using multilingual correlation》），但是没有现成模型的效果好。</p>
<p>采用了aligned word embedding（publicly available）之后，原来的基础模型就能够先在英语语料上进行训练，然后Zero-Shot地迁移到目标语言上进行测试了。如果还有目标语言的少数语料，还可以接着在目标语言的语料上进行训练。在训练的过程中，为了保证预训练的aligned word embedding的双语言对齐特性，这些参数是不进行更新的。</p>
<h3 id="双语言共同训练"><a class="markdownIt-Anchor" href="#双语言共同训练"></a> 双语言共同训练</h3>
<p>另外一种方式是可以通过aligned word embedding，将双语言（英语和目标语言）语料随机混合，共同参与训练，也就是不再分前后的顺序了。模型如下：<br>
<img src="https://i.loli.net/2019/05/09/5cd3a1541cfb4.png" alt="双语言混合训练模型"></p>
<p>相较于基础模型，这里额外添加了一个向量<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mover accent="true"><mi>k</mi><mo stretchy="true">→</mo></mover></mrow><annotation encoding="application/x-tex">\overrightarrow{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.21644em;vertical-align:0em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.21644em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span></span></span><span class="svg-align" style="top:-3.69444em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="height:0.522em;min-width:0.888em;"><svg width="400em" height="0.522em" viewbox="0 0 400000 522" preserveaspectratio="xMaxYMin slice"><path d="M0 241v40h399891c-47.3 35.3-84 78-110 128
-16.7 32-27.7 63.7-33 95 0 1.3-.2 2.7-.5 4-.3 1.3-.5 2.3-.5 3 0 7.3 6.7 11 20
 11 8 0 13.2-.8 15.5-2.5 2.3-1.7 4.2-5.5 5.5-11.5 2-13.3 5.7-27 11-41 14.7-44.7
 39-84.5 73-119.5s73.7-60.2 119-75.5c6-2 9-5.7 9-11s-3-9-9-11c-45.3-15.3-85
-40.5-119-75.5s-58.3-74.8-73-119.5c-4.7-14-8.3-27.3-11-40-1.3-6.7-3.2-10.8-5.5
-12.5-2.3-1.7-7.5-2.5-15.5-2.5-14 0-21 3.7-21 11 0 2 2 10.3 6 25 20.7 83.3 67
 151.7 139 205zm0 0v40h399900v-40z"/></svg></span></span></span></span></span></span></span></span></span>用来区分当前的训练文本属于哪一个语言（英语OR目标语言）。它将会与基础模型softmax层的输入拼接后再通过softmax层。</p>
<h2 id="实验"><a class="markdownIt-Anchor" href="#实验"></a> 实验</h2>
<h3 id="数据来源"><a class="markdownIt-Anchor" href="#数据来源"></a> 数据来源</h3>
<p>实验在两种相对低资源的语种上进行了训练：印地语（Hindi）和土耳其语（Turkish），其数据来源于对ATIS数据集的翻译工作。目标语言的训练集即随机翻译ATIS的部分训练集样本；目标语言的测试集即随机翻译ATIS的测试集样本。在对比试验（即传统的基于机器翻译的迁移）使用的翻译系统为谷歌翻译。</p>
<h3 id="实验结果"><a class="markdownIt-Anchor" href="#实验结果"></a> 实验结果</h3>
<p>Slot-filling的F1结果如下图：<br>
<img src="https://i.loli.net/2019/05/09/5cd3b9ab4552a.png" alt="Slot Filling任务的F1结果"><br>
这里采用了上面所说的三种迁移方法作为比较，分别是Train On Target(TrainOnTrg)、Test On Source(TestOnSrc)、以及上面提到过的Test On Source的一个变体(Adapt-TestOnSrc)，它们均在ATIS的全训练集上（约5k样本）进行训练。</p>
<p>Intent-classification的准确率如下：<br>
<img src="https://i.loli.net/2019/05/09/5cd3bb360ded3.png" alt="Intent Classification的准确率"><br>
这里只比较了Naive的方法（只使用目标语言的数据，通过基础模型进行训练）和Bilingual方法（双语言混合训练的方法）。可以看到Naive需要600样本达到的水平，Bilingual方法在少于100样本时就能达到。</p>
<p>Slot-filling任务几个slot的结果分析：<br>
<img src="https://i.loli.net/2019/05/09/5cd3bb1206617.png" alt="20190509133056.png"><br>
这里只使用100条目标样本。左边两个slot是高频slot，右边的三个slot是低频slot，可以看到bilingual方法相比naive方法在低频slot的识别效果上有更大的提升。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">yym6472</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://yym6472.github.io/2019/05/09/《Almost-Zero-Shot-Cross-Lingual-Spoken-Language-Understanding》阅读笔记/">https://yym6472.github.io/2019/05/09/《Almost-Zero-Shot-Cross-Lingual-Spoken-Language-Understanding》阅读笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/Transfer-Learning/">Transfer Learning</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/05/11/《Ordered-Neurons-Integrating-Tree-Structures-into-Recurrent-Neural-Networks》阅读笔记/"><i class="fa fa-chevron-left">  </i><span>《Ordered Neurons：Integrating Tree Structures into Recurrent Neural Networks》阅读笔记</span></a></div><div class="next-post pull-right"><a href="/2019/05/02/tex学习笔记/"><span>Tex学习笔记</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '1470654ab571587d5962',
  clientSecret: 'dd25cc8b92dbedf931e5a5e29b75f6bdbcff11ed',
  repo: 'yym6472.github.io',
  owner: 'yym6472',
  admin: 'yym6472',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(https://i.loli.net/2019/04/27/5cc456373162f.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By yym6472</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/algolia.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>