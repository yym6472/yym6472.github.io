<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="《Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog》阅读笔记"><meta name="keywords" content="NLP,Transfer Learning,Novel Structures,Spoken Language Understanding"><meta name="author" content="yym6472"><meta name="copyright" content="yym6472"><title>《Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog》阅读笔记 | yym6472's Blog</title><link rel="shortcut icon" href="/favicon-256.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.6.1"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"G2KYQO0W55","apiKey":"f8377236a1dbdcb6cfe6bcce269cbc82","indexName":"blog","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#简介"><span class="toc-number">1.</span> <span class="toc-text"> 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#提出的数据集"><span class="toc-number">2.</span> <span class="toc-text"> 提出的数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nlu模型"><span class="toc-number">3.</span> <span class="toc-text"> NLU模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#基础nlu模型"><span class="toc-number">3.1.</span> <span class="toc-text"> 基础NLU模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#预训练的编码器模型"><span class="toc-number">3.2.</span> <span class="toc-text"> 预训练的编码器模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#预训练的细节"><span class="toc-number">3.3.</span> <span class="toc-text"> 预训练的细节</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实验及结果分析"><span class="toc-number">4.</span> <span class="toc-text"> 实验及结果分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#实验一跨语言学习的不同迁移方法"><span class="toc-number">4.1.</span> <span class="toc-text"> 实验一：跨语言学习的不同迁移方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#实验二zero-shot的学习及其学习曲线"><span class="toc-number">4.2.</span> <span class="toc-text"> 实验二：Zero-shot的学习，及其学习曲线</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#未来可能的方向"><span class="toc-number">5.</span> <span class="toc-text"> 未来可能的方向</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://i.loli.net/2019/04/27/5cc4218c74e0e.jpg"></div><div class="author-info__name text-center">yym6472</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/yym6472">Follow Me on GitHub</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">9</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">12</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">5</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://molunerfinn.com/">MARKSZのBlog (Theme Auther's Blog)</a><a class="author-info-links__name text-center" href="https://pris-nlp.github.io/">BUPT PRIS Lab, NLP Group</a><a class="author-info-links__name text-center" href="https://helicqin.github.io/">Helicqin's Blog</a><a class="author-info-links__name text-center" href="https://hexo.io/">Hexo</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2019/04/27/5cc456373162f.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">yym6472's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a><a class="site-page" href="/contact">Contact</a></span></div><div id="post-info"><div id="post-title">《Cross-Lingual Transfer Learning for Multilingual Task Oriented Dialog》阅读笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-05-19</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习/">学习</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/学习/论文阅读笔记/">论文阅读笔记</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2.2k</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h2 id="简介"><a class="markdownIt-Anchor" href="#简介"></a> 简介</h2>
<p>本文主要想解决的问题是SLU（Spoken Language Understanding）的跨语言迁移，即从高资源语言SLU迁移到低资源语言SLU。本文的贡献有：</p>
<ul>
<li>收集了新的一个跨语言（英语、西班牙语和泰语，其中英语为源语言，西班牙语和泰语为目标低资源语言）、多任务（三种任务：Weather、Alarm、Reminder）的数据集</li>
<li>提出了一种基于预训练编码器的跨语言迁移方法</li>
<li>基于该数据集，在传统的两个方法（翻译训练数据、使用跨语言词向量编码）和本文提出的基于预训练编码器的迁移方法上进行了实验</li>
</ul>
<a id="more"></a>
<h2 id="提出的数据集"><a class="markdownIt-Anchor" href="#提出的数据集"></a> 提出的数据集</h2>
<p>数据集发布在<a href="https://fb.me/multilingual_task_oriented_data" target="_blank" rel="noopener">https://fb.me/multilingual_task_oriented_data</a>。该数据集的统计数据如下图所示：<br>
<img src="https://i.loli.net/2019/05/19/5ce117bfac6c396045.png" alt="数据集统计数据"></p>
<p>数据集的收集过程为：首先让英语母语者为每一个intent生产句子（比如：会怎么样询问天气），这样收集了43000条英语句子；随后让两个标注者基于此标注intent和slots，如果两者有分歧，则请求第三个标注着来做最终裁定；对于西班牙语和泰语的样本，则是让一小部分母语者将英语的一部分样本（随机采样）翻译成对应的语言；对于西班牙语的标注同上，出现分歧时就让第三个同时精通于英语和西班牙语的标注着进行裁定；而对于泰语，由于找不到同时精通于英语和泰语的标注者，所以处理方式就是直接丢弃掉出现标注分歧的样本。</p>
<h2 id="nlu模型"><a class="markdownIt-Anchor" href="#nlu模型"></a> NLU模型</h2>
<h3 id="基础nlu模型"><a class="markdownIt-Anchor" href="#基础nlu模型"></a> 基础NLU模型</h3>
<p>本文的基础NLU模型主要包含两个部分：</p>
<ol>
<li>首先会让给定句子通过一个句子分类器，去识别该句子属于哪个领域（Alarm、Reminder、Weather之一）</li>
<li>让这一个特定领域的模型去联合预测intent和slots</li>
</ol>
<p><img src="https://i.loli.net/2019/05/19/5ce1340da71d357848.png" alt="基础NLU模型图"><br>
上图展示了基础模型的结构：首先每个单词会被编码成一个向量，随后每个时间步的向量通过一层双向LSTM。对于意图识别而言，所有时间步的隐层向量会经过一个self-attention，然后被丢入softmax层去预测这句话的意图；对于槽填充，会将每个时间步的隐层向量经过softmax层后丢入CRF层进行预测。</p>
<p>而实验主要对照的点在于<strong>词向量编码层</strong>，可选方案如下：</p>
<ul>
<li><strong>Zero embeddings</strong>: 即使用一个词向量矩阵随任务进行训练，这个矩阵将会在一开始被初始化为零</li>
<li><strong>XLU embeddings</strong>: 使用预训练好的一个跨语言的词向量矩阵（称为XLU embeddings，见引文：<a href="https://arxiv.org/pdf/1706.04902" target="_blank" rel="noopener">Ruder et al., 2017, A survey of cross-lingual word embedding models</a>）编码一个词，并与上述随任务训练的zero embeddings进行拼接，这里的跨语言词向量矩阵是fixed的</li>
<li><strong>Encoder embeddings</strong>: 使用一个通过某种方法（后文会介绍）预训练的双向LSTM句子编码器，提取其最上层的隐层向量作为该句子中每个词语的表示，并且将这些向量与随任务联合训练的zero embeddings拼接，作为这个词最后的表示</li>
</ul>
<h3 id="预训练的编码器模型"><a class="markdownIt-Anchor" href="#预训练的编码器模型"></a> 预训练的编码器模型</h3>
<p>上文提到的encoder embeddings需要用到一个预训练的双向LSTM句子编码器。在本文的所有实验中，均采用了两层的双向LSTM编码器。实验比较了以下三种具体的策略（模型结构和预训练目标）：</p>
<ul>
<li><strong>CoVe</strong>: 按照引文<a href="https://arxiv.org/abs/1708.00107" target="_blank" rel="noopener">McCann et al, 2017, Learned in translation: Contextualized word vectors</a>，训练一个机器翻译模型，将低资源的语言（西班牙语或泰语）翻译成英语，然后将机器翻译模型中的编码器作为句子编码器</li>
<li><strong>Multilingual CoVe</strong>: 训练一个机器翻译模型，能够同时将低资源的语言翻译成英语和把英语翻译成低资源语言，模型的翻译方向取决于解码器的第一个和目标语言相关的输入token（详细见引文<a href="http://www.aclweb.org/anthology/W18-3023" target="_blank" rel="noopener">Yu et al., 2018a, Multilingual seq2seq training with similarity loss for cross-lingual document classification</a>）。在预训练这一模型的过程中，编码器是语言不可知的（即编码器无法获知所翻译的句子具体属于何种语言），因此可以期望模型学到跨语言的语义特征</li>
<li><strong>Multilingual CoVe w/ autoencoder</strong>: 作者使用的是一个双向的机器翻译模型，同时联合了自编解码器的训练目标。比如对西班牙语-英语的句子对而言：给定西班牙语的输入句子，模型会根据解码器输入的第一个token，要么生成对应的英语翻译，要么生成这个句子本身。给定英语输入句子也同样，解码器应该根据第一个token，要么输出其对应的西班牙语翻译，要么重现这个句子本身。这样设计训练目标的动机是：让编码器学习到泛化能力更强的跨语言的语义表示，因为这里和上一种训练方式不同，输入句子的语言并不决定输出句子的语言</li>
</ul>
<p>此外，对于西班牙语，作者还使用了预训练的ELMo编码作为对照，但西班牙语的ELMo编码相当于仅仅是在西班牙语的语料上进行预训练的，所以它并不是跨语言的编码。</p>
<h3 id="预训练的细节"><a class="markdownIt-Anchor" href="#预训练的细节"></a> 预训练的细节</h3>
<p>这里作者介绍了预训练上述三种机器翻译模型的一些参数设定（不同层级的向量维度等）、预训练的数据（即机器翻译的平行数据）、以及训练设定（优化器、学习率及其变化、训练代数等细节），见论文原文第四页，这里不再写出。下图给出的是各个预训练模型的perplexity指标：<br>
<img src="https://i.loli.net/2019/05/19/5ce1427c2f50164490.png" alt="预训练模型的perplexity指标"></p>
<p>其中perplexity指标是用来评价基于概率的生成模型，其生成样本好坏程度的一个方法，见<a href="https://en.wikipedia.org/wiki/Perplexity" target="_blank" rel="noopener">维基百科</a>。</p>
<h2 id="实验及结果分析"><a class="markdownIt-Anchor" href="#实验及结果分析"></a> 实验及结果分析</h2>
<h3 id="实验一跨语言学习的不同迁移方法"><a class="markdownIt-Anchor" href="#实验一跨语言学习的不同迁移方法"></a> 实验一：跨语言学习的不同迁移方法</h3>
<p>作者首先在以下设定下使用基础模型进行了实验：</p>
<ul>
<li><strong>Target only</strong>: 只使用低资源的目标语言作为训练样本</li>
<li><strong>Target only with encoder embeddings</strong>: 只使用低资源的目标语言作为训练样本，但是其编码层采用预训练的encoder embeddings</li>
<li><strong>Translate train</strong>: 将英语的训练样本翻译到目标语言，并与目标语言的训练样本融合，其中机器翻译采用的是Facebook的机器翻译系统，标注的slot信息通过attention权重（引文：<a href="https://www.aclweb.org/anthology/H01-1035" target="_blank" rel="noopener">Yarowsky et al., 2001, Inducing multilingual text analysis tools via robust projection across aligned corpora</a>）映射到翻译后的句子</li>
<li><strong>Cross-lingual with XLU embeddings</strong>: 将英语和目标语言的训练样本混合后进行训练，并采用XLU embeddings编码token，其中XLU embeddings采用的是预训练的MUSE跨语言编码（引文：<a href="https://arxiv.org/abs/1710.04087" target="_blank" rel="noopener">Conneau et al., 2017, Word translation without parallel data</a>），由于该编码没有对泰语的版本，所以只在西班牙语上进行了实验</li>
<li><strong>Cross-lingual with encoder embeddings</strong>: 将英语和目标语言的训练样本混合后进行训练，并采用上述的三种encoder embeddings编码token，同时作为对照的ELMo编码也会在西班牙语上进行实验</li>
</ul>
<p>实验结果如下图所示：<br>
<img src="https://i.loli.net/2019/05/19/5ce150242456e17293.png" alt="第一个实验的实验结果"><br>
TODO: 作者对其的具体分析</p>
<h3 id="实验二zero-shot的学习及其学习曲线"><a class="markdownIt-Anchor" href="#实验二zero-shot的学习及其学习曲线"></a> 实验二：Zero-shot的学习，及其学习曲线</h3>
<p>从上一个实验看来，单单使用全部的目标语言训练样本，无法清楚的看出跨语言的编码是否有帮助。因此作者做了第二个实验，也就是使用更少的数据量的学习。</p>
<p>首先是zero-shot的实验结果，即：不使用任何目标语言训练样本，单单采用英语的训练数据，其实验结果如下图：<br>
<img src="https://i.loli.net/2019/05/19/5ce15657f253e62868.png" alt="Zero-shot实验结果"><br>
可以看到预训练的两种跨语言CoVe编码是超过单语言的CoVe编码的，证明了跨语言的编码相比单语言编码还是有所帮助的，同时可以在西班牙语的实验结果上看到这两种跨语言的CoVe编码，在zero-shot的情况下比XLU embeddings的效果更好。另外，意料之中，机器翻译（Translate train）的方法在zero-shot时取得了最好的效果。</p>
<p>其次是在有限的目标语言（few-shot）训练数据下，模型的性能表现如下：<br>
<img src="https://i.loli.net/2019/05/19/5ce15797f107f25400.png" alt="不同数量的目标语言训练样本下的实验结果"><br>
TODO: 作者的具体分析</p>
<h2 id="未来可能的方向"><a class="markdownIt-Anchor" href="#未来可能的方向"></a> 未来可能的方向</h2>
<p>TODO</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">yym6472</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://yym6472.github.io/2019/05/19/《Cross-Lingual-Transfer-Learning-for-Multilingual-Task-Oriented-Dialog》阅读笔记/">https://yym6472.github.io/2019/05/19/《Cross-Lingual-Transfer-Learning-for-Multilingual-Task-Oriented-Dialog》阅读笔记/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/Transfer-Learning/">Transfer Learning</a><a class="post-meta__tags" href="/tags/Novel-Structures/">Novel Structures</a><a class="post-meta__tags" href="/tags/Spoken-Language-Understanding/">Spoken Language Understanding</a></div><div class="social-share"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><nav id="pagination"><div class="next-post pull-right"><a href="/2019/05/11/《Ordered-Neurons-Integrating-Tree-Structures-into-Recurrent-Neural-Networks》阅读笔记/"><span>《Ordered Neurons：Integrating Tree Structures into Recurrent Neural Networks》阅读笔记</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '1470654ab571587d5962',
  clientSecret: 'dd25cc8b92dbedf931e5a5e29b75f6bdbcff11ed',
  repo: 'yym6472.github.io',
  owner: 'yym6472',
  admin: 'yym6472',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(https://i.loli.net/2019/04/27/5cc456373162f.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 By yym6472</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fas fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/algolia.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>if(/Android|webOS|iPhone|iPod|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>